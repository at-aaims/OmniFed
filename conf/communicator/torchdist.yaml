# ══════════════════════════════════════════════════════════════════════════════
# TorchDist Communicator Configuration
#
# PyTorch distributed communication backend.
# Uses torch.distributed primitives for broadcast and aggregation.
# Supports TCP and file-based initialization.
# ══════════════════════════════════════════════════════════════════════════════

_target_: src.flora.communicator.TorchDistCommunicator

# ─────────────────────────────────────────
# Connection Settings (usually auto-configured by topology)
# rank: ??? # Node rank (set by topology)
# world_size: ??? # Total nodes (set by topology)

# -------------------
# Network Configuration
init_method: tcp # tcp or file
master_addr: 127.0.0.1 # Master node address
master_port: "29500" # Master node port
backend: gloo # gloo (CPU) or nccl (GPU)

# -------------------
# Advanced Options
# sharedfile: sharedfile # Path for file-based init
# timeout: 60 # Connection timeout in seconds
# max_retries: 5 # Connection retry attempts