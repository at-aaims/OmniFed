# ══════════════════════════════════════════════════════════════════════════════
# FLORA Base Configuration
#
# Foundation config for all FLORA experiments.
# Child configs inherit this structure via defaults list.
#
# Usage: Add "- base" to your defaults list, then override what you need.
# Architecture: algorithm + model + datamodule + topology + schedules
#
# Docs: https://hydra.cc/docs/patterns/extending_configs/
#       https://hydra.cc/docs/advanced/defaults_list/
# ══════════════════════════════════════════════════════════════════════════════

defaults:
  - _self_

# ─────────────────────────────────────────
# Required Components (override in child configs)

global_rounds: ??? # Number of FL rounds

# -------------------
# Topology

topology:
  _target_: ??? # Full dotpath to topology class

# -------------------
# Algorithm

algorithm:
  _target_: ??? # Full dotpath to algorithm class
  local_lr: ??? # Learning rate for local training
  max_epochs_per_round: ??? # Local epochs per FL round

# -------------------
# Model

model:
  _target_: ??? # Full dotpath to model class

# -------------------
# DataModule

datamodule:
  _target_: src.flora.data.DataModule
  train:
    _target_: torch.utils.data.DataLoader
    batch_size: ??? # Training batch size
  eval:
    _target_: torch.utils.data.DataLoader
    batch_size: ??? # Eval batch size

# ─────────────────────────────────────────
# Schedules Configuration
#
# Controls when aggregation and evaluation happen.
# Options: true/false, every: N, at: [X, Y], or combined patterns
schedules:
  _target_: src.flora.utils.ExecutionSchedules
  aggregation:
    _target_: src.flora.utils.AggregationTriggers
    round_end: true # Aggregate every round (standard FL)
    # round_end: false           # Never aggregate at round end
    # round_end: null            # Explicitly disabled (same as false)
    # round_end:                 # Custom frequency
    #   every: 2                 # Every 2nd round
    # round_end:                 # Specific milestones
    #   at: [0, 5, 10]          # Only at rounds 0, 5, and 10

    epoch_end: false
    # epoch_end: true            # Every epoch
    # epoch_end:                 # Custom patterns
    #   every: 3                 # Every 3rd epoch
    #   at: [1, 4]              # Also at epochs 1 and 4

    batch_end: false
    # batch_end:                 # High-frequency aggregation
    #   every: 100               # Every 100 batches

  evaluation:
    _target_: src.flora.utils.EvaluationTriggers
    experiment_start: true
    # experiment_start: true     # Evaluate at experiment start

    experiment_end: true
    # experiment_end: true       # Evaluate at experiment end

    pre_aggregation: false
    # pre_aggregation: true      # Before every aggregation
    # pre_aggregation:           # Selective evaluation
    #   every: 2                 # Before every 2nd aggregation

    post_aggregation: true # Evaluate after every aggregation
    # post_aggregation: false
    # post_aggregation:          # Custom evaluation schedule
    #   at: [0, 1, 5]           # After 1st, 2nd, and 6th aggregations

# ─────────────────────────────────────────
# Hydra Config
# Docs:
# - https://hydra.cc/docs/configure_hydra/intro/
# - https://hydra.cc/docs/configure_hydra/job/

hydra:
  job:
    env_set:
      # Environment variables for the Hydra job
      # These can be used to configure the runtime environment for FL workers.
      # Note: These variables will be copied to all FL workers

      FLORA_ENV_DUMMY1: "dummy_value1" # Example variable for demonstration

      # Example settings:

      # GPU & threading:
      # CUDA_VISIBLE_DEVICES: "0,1"        # Assign specific GPUs
      # OMP_NUM_THREADS: 4                 # Limit CPU threads per client

      # PyTorch debugging:
      # NCCL_DEBUG: INFO                   # GPU communication debug
      # TORCH_DISTRIBUTED_DEBUG: DETAIL    # Distributed training debug
      # TORCH_CPP_LOG_LEVEL: INFO          # PyTorch C++ logs

      # Framework logging:
      # TF_CPP_MIN_LOG_LEVEL: 3            # Suppress TensorFlow warnings
      # GRPC_VERBOSITY: ERROR              # Minimize gRPC logging

    # Copy local environment variables to FL workers:
    # env_copy:
    #   - WANDB_API_KEY # W&B experiment tracking

  run:
    # Output directory for FL experiment results and logs
    # Docs: https://hydra.cc/docs/configure_hydra/workdir/
    dir: outputs/${now:%Y-%m-%d}/${hydra.job.config_name}

    # Alternative example patterns:
    # dir: outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}              # Group by date/time
    # dir: outputs/${hydra.job.name}/${now:%Y-%m-%d_%H-%M-%S}   # Group by job name
    # dir: outputs/${algorithm._target_}/${now:%Y-%m-%d}        # Group by algorithm
