# ══════════════════════════════════════════════════════════════════════════════
# FLORA Base Configuration
#
# Base template with required components for federated learning experiments.
# ══════════════════════════════════════════════════════════════════════════════

defaults:
  - topology: base
  - algorithm: base
  - model: base
  - datamodule: base
  - _self_

# ─────────────────────────────────────────
# Experiment Settings

global_rounds: ??? # Number of federated learning rounds to execute

# ─────────────────────────────────────────
# Ray Cluster Configuration
# Controls distributed compute infrastructure for federated learning
# Docs: https://docs.ray.io/en/latest/ray-core/api/doc/ray.init.html

ray:
  # ─────────────────────────────────────────
  # Cluster Connection & Resource Allocation
  # ─────────────────────────────────────────
  
  # Cluster connection (null = auto-detect local cluster)
  # Use "ray://host:port" for remote clusters, "local" to force local
  address: null
  
  # Resource allocation - CRITICAL for proper GPU/CPU distribution
  # null = auto-detect based on hardware, explicit numbers override detection
  num_cpus: null                   
  num_gpus: null                   
  resources: {}                    # Custom resources: {"accelerator_type": "V100", "high_memory": 2}
  
  # GPU allocation: Automatic (shares GPUs when needed, 1 per actor otherwise)
  # Override when needed: ray_actor_options.num_gpus in topology configs
  # Debug: Check [GPU-AUTO] logs during startup
  
  # ─────────────────────────────────────────
  # Memory & Performance
  # ─────────────────────────────────────────
  
  # Object store memory for large model sharing (null = 30% of system memory)
  object_store_memory: null
  
  # ─────────────────────────────────────────
  # Monitoring & Development
  # ─────────────────────────────────────────
  
  # Essential for FL: forward all distributed node logs to main process
  log_to_driver: true
  
  # Ray dashboard (null = auto-start if dependencies available)
  include_dashboard: null
  dashboard_host: "127.0.0.1"     # Use "0.0.0.0" for external access
  dashboard_port: null             # null = auto-find port starting from 8265
  
  # Development convenience - allow multiple ray.init() calls without error
  ignore_reinit_error: true
  
  # ─────────────────────────────────────────
  # Advanced Configuration
  # ─────────────────────────────────────────
  
  # Experiment isolation (null = anonymous namespace)
  namespace: null
  
  # Runtime environment for distributed workers (empty = inherit from main process)
  runtime_env: {}                  # Example: {"pip": ["torch==1.12.0"], "env_vars": {"CUDA_VISIBLE_DEVICES": "0,1"}}
  
  # Commented advanced options (uncomment if needed):
  # object_spilling_directory: null    # Directory for object spilling (null = system temp)
  # labels: {}                         # [Experimental] Node labels: {"node_type": "gpu_worker"}
  # configure_logging: true            # Whether Ray can configure logging
  # logging_level: null                # Ray logging level (null = INFO)
  # enable_resource_isolation: false   # Use cgroups for resource isolation

# ─────────────────────────────────────────
# Hydra Configuration
# Docs:
# - https://hydra.cc/docs/configure_hydra/intro/
# - https://hydra.cc/docs/configure_hydra/job/

hydra:
  job:
    env_set:
      # Environment variables for the Hydra job
      # These can be used to configure the runtime environment for FL workers.
      # Note: These variables will be copied to all FL workers

      FLORA_ENV_DUMMY1: "dummy_value1" # Example variable for demonstration

      # Example settings:

      # GPU & threading:
      # CUDA_VISIBLE_DEVICES: "0,1"        # Assign specific GPUs
      # OMP_NUM_THREADS: 4                 # Limit CPU threads per client

      # PyTorch debugging:
      # NCCL_DEBUG: INFO                   # GPU communication debug
      # TORCH_DISTRIBUTED_DEBUG: DETAIL    # Distributed training debug
      # TORCH_CPP_LOG_LEVEL: INFO          # PyTorch C++ logs

      # Framework logging:
      # TF_CPP_MIN_LOG_LEVEL: 3            # Suppress TensorFlow warnings
      # GRPC_VERBOSITY: ERROR              # Minimize gRPC logging

    # Copy local environment variables to FL workers:
    # env_copy:
    #   - WANDB_API_KEY # W&B experiment tracking

  run:
    # Output directory for FL experiment results and logs
    # Docs: https://hydra.cc/docs/configure_hydra/workdir/
    dir: outputs/${now:%Y-%m-%d}/${hydra.job.config_name}

    # Alternative example patterns:
    # dir: outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}              # Group by date/time
    # dir: outputs/${hydra.job.name}/${now:%Y-%m-%d_%H-%M-%S}   # Group by job name
    # dir: outputs/${algorithm._target_}/${now:%Y-%m-%d}        # Group by algorithm
