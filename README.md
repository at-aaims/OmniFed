# FLUX

A federated learning framework built on [Ray](https://ray.io/) and [Hydra](https://hydra.cc/). FLUX scales from local experiments to HPC clusters and cross-institutional scenarios with 11 built-in algorithms and extensible architecture.

## Key Features

- **ğŸ§© Modular**: Mix and match 11 single-file algorithm implementations, topologies, and communication protocols
- **ğŸ“Š Flexible**: Local, HPC, and cross-network deployments with multiple communication backends
- **âš™ï¸ Extensible**: Custom algorithms, communicators, and topologies with minimal code requirements
- **ğŸ”¬ Research-Friendly**: Easy experimentation with lifecycle hooks and [PyTorch](https://pytorch.org/) compatibility
- **ğŸš€ Scalable**: [Ray](https://ray.io/)-powered distributed coordination from laptops to HPC clusters

## Quick Start

```bash
# Clone and install
git clone <repository-url>
cd FLUX
pip install -r requirements.txt

# Run basic federated learning experiment
./main.sh --config-name test_fedavg_centralized_torchdist
```

This configures a federated learning experiment with multiple nodes using the CIFAR-10 dataset. You'll see:

- **Setup phase**: Ray cluster initialization, node actor creation, and model broadcasting
- **Training progress**: Loss, accuracy, and other metrics logged per batch/epoch  
- **Communication logs**: Model aggregation and synchronization between nodes
- **Results**: Final metrics saved to timestamped output directory

## Running Experiments

**Different deployment types:**

```bash
# Local/HPC clusters (PyTorch distributed communication)
./main.sh --config-name test_fedavg_centralized_torchdist

# Cross-network deployment (gRPC communication)
./main.sh --config-name test_fedavg_centralized_grpc

# Multi-tier hierarchical setup
./main.sh --config-name test_fedavg_hierarchical
```

**Customize parameters:**

```bash
# Override any parameter
./main.sh --config-name test_fedavg_centralized_torchdist topology.num_clients=10 global_rounds=10 algorithm.max_epochs_per_round=8
```

## Configuration

**Explore available configurations:**

```bash
# See all available experiment configs
python main.py --help
```

**Inspect configurations:**

```bash
# Preview full configuration before running
python main.py --config-name test_fedavg_centralized_torchdist --cfg job

# Show resolved configuration (with interpolations)
python main.py --config-name test_fedavg_centralized_torchdist --cfg job --resolve

# Focus on specific config sections
python main.py --config-name test_fedavg_centralized_torchdist --cfg job --package algorithm
```

**Troubleshooting:**

```bash
# Get detailed Hydra system information
python main.py --info
```

See [Hydra's command line flags](https://hydra.cc/docs/advanced/hydra-command-line-flags/) for more configuration options.

## How FLUX Works

FLUX orchestrates federated learning experiments through a modular architecture:

**Core Components:**

- **Algorithm**: Defines the federated learning strategy (e.g., FedAvg for simple averaging, SCAFFOLD for drift correction, MOON for contrastive learning)
- **Topology**: Specifies the network structure and client-server relationships (centralized for star topology, hierarchical for multi-tier deployments)
- **Communicator**: Handles message passing between nodes (PyTorch distributed for HPC clusters, gRPC for cross-network scenarios)
- **DataModule**: Manages data loading, partitioning, and distribution across clients
- **Model**: Any PyTorch nn.Module with seamless integration

**Execution Flow:**

1. **Initialization**: Ray spawns distributed actors based on topology configuration
2. **Local Training**: Each client trains on private data for specified epochs/batches
3. **Model Exchange**: Clients send updates to aggregators via the communicator
4. **Aggregation**: Server combines updates using the algorithm's aggregation strategy
5. **Model Distribution**: Updated global model is broadcast back to clients
6. **Evaluation**: Periodic validation on local and/or global test sets

**Additional Capabilities:**

- **Flexible Scheduling**: Control when aggregation and evaluation occur
- **Metric Tracking**: Built-in logging system for training loss and custom metrics
- **Stateful Algorithms**: Support for momentum, control variates, and personalized models

## Project Structure

```
FLUX/
â”œâ”€â”€ src/flora/              # Main framework code
â”‚   â”œâ”€â”€ algorithm/          # Federated learning algorithms
â”‚   â”‚   â”œâ”€â”€ base.py         # Base algorithm class
â”‚   â”‚   â”œâ”€â”€ fedavg.py       # FedAvg
â”‚   â”‚   â””â”€â”€ ...             # 10 more algorithms (SCAFFOLD, MOON, FedProx, etc.)
â”‚   â”œâ”€â”€ communicator/       # Communication protocols
â”‚   â”‚   â”œâ”€â”€ base.py         # Base communicator class
â”‚   â”‚   â”œâ”€â”€ torchdist.py    # PyTorch distributed backend
â”‚   â”‚   â”œâ”€â”€ grpc.py         # gRPC backend
â”‚   â”‚   â””â”€â”€ ...             # gRPC server/client implementations
â”‚   â”œâ”€â”€ topology/           # Network structures
â”‚   â”‚   â”œâ”€â”€ base.py         # Base topology class
â”‚   â”‚   â”œâ”€â”€ centralized.py  # Centralized topology
â”‚   â”‚   â”œâ”€â”€ hierarchical.py # Hierarchical topology
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ model/              # Built-in model examples and reusable components
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ data/               # Data loading and partitioning
â”‚   â”‚   â”œâ”€â”€ datamodule.py   # DataModule class
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ utils/              # Utilities and helpers
â”‚   â”‚   â”œâ”€â”€ metric_logger.py    # Metrics tracking and logging
â”‚   â”‚   â”œâ”€â”€ results_display.py  # Results visualization
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ engine.py           # Ray orchestration and coordination
â”‚   â””â”€â”€ node.py             # Federated learning participant actors
â”œâ”€â”€ conf/                   # Hydra configuration files
â”‚   â”œâ”€â”€ algorithm/          # Algorithm-specific configs
â”‚   â”œâ”€â”€ datamodule/         # Dataset configurations
â”‚   â”œâ”€â”€ model/              # Model architecture configs
â”‚   â”œâ”€â”€ topology/           # Network topology configs
â”‚   â””â”€â”€ test_*.yaml         # Example experiment configurations
â”œâ”€â”€ main.py                 # Python entry point
â”œâ”€â”€ main.sh                 # Development script with setup handling
â””â”€â”€ requirements.txt        # Dependencies
```

## Contributing

1. Fork the repository
2. Create feature branch: `git checkout -b feature-name`
3. Test your changes
4. Submit pull request

## Citation

```bibtex
@inproceedings{flux2025,
  title={FLUX: A Modular Federated Learning Framework},
  author={Authors},
  year={2025}
}
```
